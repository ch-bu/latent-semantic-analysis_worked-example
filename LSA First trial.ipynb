{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSA with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import TfidfModel\n",
    "import re\n",
    "import gensim\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and make dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Gutenberg data\n",
    "with open('datasets/gutenberg.txt', 'r') as gutenberg:\n",
    "    sherlock = gutenberg.read().replace('\\n', ' ')\n",
    "    \n",
    "sherlock_sentences = sent_tokenize(sherlock)\n",
    "\n",
    "documents = [[re.sub(r'[^\\w]', ' ', word).lower().encode('utf-8') for word\n",
    "              in sentence.split() if word not in stopwords.words('english')]\n",
    "              for sentence in sherlock_sentences]\n",
    "\n",
    "# Remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for sentence in documents:\n",
    "    for token in sentence:\n",
    "        frequency[token] += 1\n",
    "              \n",
    "documents = [[token for token in sentence if frequency[token] > 1] \n",
    "             for sentence in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "dictionary.save('/tmp/gutenberg.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(6 unique tokens: [u'load', u'used', u'theory', u'sweller', u'john']...)\n",
      "[['cognitive', 'cognitive', 'load', 'used'], ['cognitive', 'load', 'theory', 'john', 'sweller'], ['sweller', 'used', 'cognitive', 'load'], ['cognitive', 'load', 'theory', 'cognitive', 'load'], ['john']]\n",
      "[(0, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "print dictionary\n",
    "print documents\n",
    "\n",
    "# The function doc2bow converts an array to a bag of words format\n",
    "print dictionary.doc2bow([\"load\", \"theory\", \"to\", \"the\", \"garden\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Build corpus. Convert dictionary entries to integer tuples\n",
    "corpus = [dictionary.doc2bow(text) for text in documents]\n",
    "\n",
    "# Storke corpus on dics\n",
    "# corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'gutenberg.mm'), corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load id-> word mapping (the dictionary)\n",
    "# id2word = gensim.corpora.Dictionary.load_from_text('/tmp/gutenberg.dict')\n",
    "\n",
    "# Load corpus iterator\n",
    "#mm = gensim.corpora.MmCorpus('/tmp/gutenberg.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build Tf-IDF model\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Durch die TfidfModel Klasse kann ich die Document Term Matrix gewichten.\n",
    "# Wichtige (infrequente) Wörter werden höher gewichtet als frequente Wörter.\n",
    "# Beide Matrizen kann ich später in das LSI(LSA) Modell hineinfügen\n",
    "tfidf = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LSA over tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LsiModel(num_terms=6, num_topics=300, decay=1.0, chunksize=20000)\n",
      "LsiModel(num_terms=6, num_topics=300, decay=1.0, chunksize=20000)\n"
     ]
    }
   ],
   "source": [
    "# Calculate LSA with both corpus (weighted and unweighted)\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "\n",
    "# Die num_topics des Models geben nichts anderes an als die Dimensionen, die\n",
    "# verwendet werden, um die Reihen der Matrix zu verkleinern. Dies findet durch\n",
    "# die Singular Value Decomposition statt. \n",
    "lsa       = LsiModel(corpus      , num_topics=300)\n",
    "lsa_tfidf = LsiModel(corpus_tfidf, num_topics=300)\n",
    "print(lsa)\n",
    "print(lsa_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarities between all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixSimilarity<5 docs, 6 features>\n"
     ]
    }
   ],
   "source": [
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "# Interestingly we can do similarities with a different corpus. Whe\n",
    "# trained the LSI-Model with our tf-idf corpus but we could use\n",
    "# just another one\n",
    "similarity = MatrixSimilarity(lsa_tfidf[corpus_tfidf], num_features=lsa_tfidf.num_terms)\n",
    "\n",
    "print similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, array([  9.99999940e-01,   8.84811655e-02,   7.10719287e-01,\n",
      "         2.57363170e-01,   3.07810361e-10], dtype=float32)), (1, array([ 0.08848117,  0.99999994,  0.43518433,  0.57696968,  0.56626439], dtype=float32)), (2, array([  7.10719287e-01,   4.35184330e-01,   1.00000012e+00,\n",
      "         1.34221151e-01,  -1.78835329e-08], dtype=float32)), (3, array([  2.57363170e-01,   5.76969683e-01,   1.34221151e-01,\n",
      "         1.00000000e+00,   3.22521032e-09], dtype=float32)), (4, array([  3.07810361e-10,   5.66264391e-01,  -1.78835329e-08,\n",
      "         3.22521032e-09,   1.00000000e+00], dtype=float32))]\n",
      "[  3.07810361e-10   5.66264391e-01  -1.78835329e-08   3.22521032e-09\n",
      "   1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities = list(enumerate(similarity))\n",
    "\n",
    "print(cosine_similarities)\n",
    "print(cosine_similarities[4][1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
