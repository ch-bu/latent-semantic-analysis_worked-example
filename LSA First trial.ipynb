{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSA with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import TfidfModel\n",
    "import re\n",
    "import gensim\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and make dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "# Read Gutenberg data\n",
    "with open('datasets/gutenberg.txt', 'r') as gutenberg:\n",
    "    sherlock = gutenberg.read().replace('\\n', ' ')\n",
    "    \n",
    "sherlock_sentences = sent_tokenize(sherlock)\n",
    "\n",
    "documents = [[re.sub(r'[^\\w]', ' ', word).lower().encode('utf-8') for word\n",
    "              in sentence.split() if word not in stopwords.words('english')]\n",
    "              for sentence in sherlock_sentences]\n",
    "\n",
    "# Remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for sentence in documents:\n",
    "    for token in sentence:\n",
    "        frequency[token] += 1\n",
    "              \n",
    "documents = [[token for token in sentence if frequency[token] > 1] \n",
    "             for sentence in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "dictionary.save('/tmp/gutenberg.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(16 unique tokens: [u'load', u'hops ', u'used', u'natural', u'cognitive']...)\n",
      "[['in', 'cognitive', 'cognitive', 'load', 'used'], ['cognitive', 'load', 'theory', 'john', 'sweller'], ['sweller', 'used', 'cognitive', 'load'], ['cognitive', 'load', 'theory', 'cognitive', 'load'], ['john'], ['beer', 'drink', 'drink'], ['beer', 'brewed'], ['carbonation', 'beer '], ['beer', 'brewed', 'hops ', 'natural'], ['included', 'used', 'hops '], ['in', 'natural', 'carbonation'], ['beer ', 'included', 'beer', 'beer', 'prayer', 'beer ', 'prayer', 'beer']]\n",
      "[(0, 1), (6, 1)]\n"
     ]
    }
   ],
   "source": [
    "print dictionary\n",
    "print documents\n",
    "\n",
    "# The function doc2bow converts an array to a bag of words format\n",
    "print dictionary.doc2bow([\"load\", \"theory\", \"to\", \"the\", \"garden\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Build corpus. Convert dictionary entries to integer tuples\n",
    "corpus = [dictionary.doc2bow(text) for text in documents]\n",
    "\n",
    "# Storke corpus on dics\n",
    "# corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'gutenberg.mm'), corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load id-> word mapping (the dictionary)\n",
    "# id2word = gensim.corpora.Dictionary.load_from_text('/tmp/gutenberg.dict')\n",
    "\n",
    "# Load corpus iterator\n",
    "#mm = gensim.corpora.MmCorpus('/tmp/gutenberg.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build Tf-IDF model\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Durch die TfidfModel Klasse kann ich die Document Term Matrix gewichten.\n",
    "# Wichtige (infrequente) Wörter werden höher gewichtet als frequente Wörter.\n",
    "# Beide Matrizen kann ich später in das LSI(LSA) Modell hineinfügen\n",
    "tfidf = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LSA over tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LsiModel(num_terms=16, num_topics=2, decay=1.0, chunksize=20000)\n",
      "LsiModel(num_terms=16, num_topics=2, decay=1.0, chunksize=20000)\n"
     ]
    }
   ],
   "source": [
    "# Calculate LSA with both corpus (weighted and unweighted)\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "\n",
    "# Die num_topics des Models geben nichts anderes an als die Dimensionen, die\n",
    "# verwendet werden, um die Reihen der Matrix zu verkleinern. Dies findet durch\n",
    "# die Singular Value Decomposition statt. \n",
    "lsa       = LsiModel(corpus      , num_topics=2)\n",
    "lsa_tfidf = LsiModel(corpus_tfidf, num_topics=2)\n",
    "print(lsa)\n",
    "print(lsa_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarities between all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixSimilarity<12 docs, 16 features>\n"
     ]
    }
   ],
   "source": [
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "# Interestingly we can do similarities with a different corpus. We\n",
    "# trained the LSI-Model with our tf-idf corpus but we could use\n",
    "# just another one\n",
    "similarity = MatrixSimilarity(lsa_tfidf[corpus_tfidf], num_features=lsa_tfidf.num_terms)\n",
    "\n",
    "print similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, array([ 1.        ,  0.96896333,  0.99281341,  0.97989964,  0.91932237,\n",
      "        0.08354242,  0.09665251,  0.16798048,  0.15166092,  0.56825316,\n",
      "        0.39827818,  0.12378429], dtype=float32)), (1, array([ 0.96896333,  1.        ,  0.99158341,  0.99880189,  0.98806596,\n",
      "       -0.16539106, -0.15239467, -0.08092515, -0.09739138,  0.34720302,\n",
      "        0.15916471, -0.12536113], dtype=float32)), (2, array([ 0.99281341,  0.99158341,  1.        ,  0.99673116,  0.95980746,\n",
      "       -0.03631249, -0.02315469,  0.04880091,  0.03228242,  0.46569604,\n",
      "        0.2856442 ,  0.00414221], dtype=float32)), (3, array([ 0.97989964,  0.99880189,  0.99673116,  1.        ,  0.97934443,\n",
      "       -0.1169306 , -0.10384742, -0.03205244, -0.04857108,  0.39267901,\n",
      "        0.20728645, -0.07666072], dtype=float32)), (4, array([ 0.91932237,  0.98806596,  0.95980746,  0.97934443,  0.99999994,\n",
      "       -0.3153272 , -0.30280808, -0.23348542, -0.2495281 ,  0.19861057,\n",
      "        0.0051976 , -0.27668115], dtype=float32)), (5, array([ 0.08354242, -0.16539106, -0.03631249, -0.1169306 , -0.3153272 ,\n",
      "        1.        ,  0.99991339,  0.99637771,  0.99764735,  0.86745054,\n",
      "        0.94733125,  0.99918151], dtype=float32)), (6, array([ 0.09665251, -0.15239467, -0.02315469, -0.10384742, -0.30280808,\n",
      "        0.99991339,  1.        ,  0.99741077,  0.99846333,  0.87392443,\n",
      "        0.95146483,  0.99962741], dtype=float32)), (7, array([ 0.16798048, -0.08092515,  0.04880091, -0.03205244, -0.23348542,\n",
      "        0.99637771,  0.99741077,  0.99999994,  0.99986333,  0.90661669,\n",
      "        0.97113359,  0.99900204], dtype=float32)), (8, array([ 0.15166092, -0.09739138,  0.03228242, -0.04857108, -0.2495281 ,\n",
      "        0.99764735,  0.99846333,  0.99986333,  1.        ,  0.8995173 ,\n",
      "        0.96705753,  0.99960393], dtype=float32)), (9, array([ 0.56825316,  0.34720302,  0.46569604,  0.39267901,  0.19861057,\n",
      "        0.86745054,  0.87392443,  0.90661669,  0.8995173 ,  1.        ,\n",
      "        0.98109752,  0.88686615], dtype=float32)), (10, array([ 0.39827818,  0.15916471,  0.2856442 ,  0.20728645,  0.0051976 ,\n",
      "        0.94733125,  0.95146483,  0.97113359,  0.96705753,  0.98109752,\n",
      "        1.        ,  0.95951074], dtype=float32)), (11, array([ 0.12378429, -0.12536113,  0.00414221, -0.07666072, -0.27668115,\n",
      "        0.99918151,  0.99962741,  0.99900204,  0.99960393,  0.88686615,\n",
      "        0.95951074,  1.        ], dtype=float32))]\n",
      "[ 0.91932237  0.98806596  0.95980746  0.97934443  0.99999994 -0.3153272\n",
      " -0.30280808 -0.23348542 -0.2495281   0.19861057  0.0051976  -0.27668115]\n",
      "[(0, '-0.749*\"7\" + -0.433*\"11\" + -0.410*\"15\" + -0.221*\"14\" + -0.092*\"8\" + -0.088*\"9\" + -0.070*\"2\" + -0.063*\"12\" + -0.058*\"0\" + -0.052*\"13\"'), (1, '-0.701*\"2\" + -0.582*\"0\" + -0.235*\"6\" + -0.215*\"1\" + -0.177*\"5\" + -0.126*\"3\" + -0.097*\"4\" + 0.080*\"7\" + 0.047*\"11\" + 0.044*\"15\"')]\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities = list(enumerate(similarity))\n",
    "\n",
    "# Obviously LSA could divide \n",
    "print(cosine_similarities)\n",
    "print(cosine_similarities[4][1])\n",
    "\n",
    "print(lsa.print_topics(num_topics=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
