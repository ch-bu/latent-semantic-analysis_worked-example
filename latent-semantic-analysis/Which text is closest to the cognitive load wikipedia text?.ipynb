{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get text clostest to the wikipedia cognitive load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.similarities import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv(\"datasets/clt.csv\", sep=\",\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle data and build corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get all texts in list\n",
    "documents = []\n",
    "\n",
    "for document in data[\"text\"]:\n",
    "    documents.append(document)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('german')\n",
    "\n",
    "tokenized_documents = []\n",
    "\n",
    "# Tokenize each document\n",
    "for index, document in enumerate(documents):\n",
    "    #tokenized_documents.append(([token.lower_ \n",
    "    #                             for token in doc if (token.is_alpha == True \n",
    "    #                                                  and token.text not in stopwords)]))\n",
    "    tokenized_documents.append([token.lower() for token in document.split()])\n",
    "\n",
    "# Build dictionary\n",
    "dictionary = corpora.Dictionary(tokenized_documents)\n",
    "\n",
    "# Build corpus\n",
    "corpus = [dictionary.doc2bow(document) for document in tokenized_documents]\n",
    "\n",
    "# Build tf-idf representation\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Change corpus according to model\n",
    "tfidf_model = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change wikipedia representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:8: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "wikipedia = \"\"\n",
    "\n",
    "with open(\"datasets/clt_wikipedia.txt\") as file:\n",
    "    for l in file:\n",
    "        wikipedia = l\n",
    "\n",
    "wikipedia_tokens = [token.lower() for token in wikipedia.split() if token not in stopwords]\n",
    "\n",
    "# Create document representation from data\n",
    "wikipedia_tuples = dictionary.doc2bow(wikipedia_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 0.57989264), (37, 0.56788445), (14, 0.52291572), (22, 0.51325983), (63, 0.50370324), (78, 0.50132543), (64, 0.49617299), (60, 0.46252349), (25, 0.45030314), (15, 0.44746932), (10, 0.44380844), (70, 0.44029707), (75, 0.432652), (76, 0.42977303), (39, 0.42936102), (7, 0.42906737), (49, 0.42705083), (67, 0.42611316), (16, 0.42075834), (52, 0.4162102)]\n",
      "Die Cognitive-Load-Theory ist eine Theorie, welche erklärt, durch welche Einflüsse das Lernen erleichtert oder erschwert werden kann. Man geht davon aus, dass der Mensch beim Lernen eine gewisse kognitive Belastung auf sich trägt, welche eine wichtige Rolle spielt. Bei der Cognitive-Load-Theory geht man davon aus, dass das Gedächtnis des Menschen in verschiedene Bereiche aufgeteilt ist. Zum Prozess der Informationsverarbeitung von neu Gelerntem, ist das Arbeitsgedächtnis sehr wichtig. Dieses ist eine Art Kurzzeitgedächtnis und ist daher sowohl zeitlich als auch von der Kapazität beschränkt. Um ein erfolgreiches Lernen zu ermöglichen, ist es wichtig, dass das Arbeitsgedächtnis nicht kognitiv überlastet wird, das heißt, dass für das Lernen nicht notwendige Informationen und Reize, die über die Sinne aufgenommen werden, möglichst vermieden werden. Durch das Arbeitsgedächtnis gelangen gelernte Informationen im besten Fall in den Langzeitspeicher und bleiben dem Lernenden durch Wiederholungen und Vertiefungen für immer erhalten.  Diese kognitive Belastung lässt sich in drei Bereiche einteilen: In den Bereich der extrinsischen Belastung kann man beispielsweise die Gestaltung des zu lernenden Materials zählen. Hierbei ist wichtig, dass möglichst wenig überflüssige Informationen vorhanden sind, welche nicht zwingend notwendig sind. Dazu gehören auch Wiederholungen oder zum Beispiel zu viele Bilder, welche den Lernenden vom eigentlichen Inhalt ablenken.  Der zweite Bereich ist die sogenannte Intrinsische-Belastung, welche angibt, wie schwierig der Inhalt des Lehrmaterials an sich ist. Je schwieriger hierbei das Lehrmaterial ist, desto höher ist die Intrinsische-Belastung.  Schließlich gibt es noch die Lernbezogene-Belastung. Darunter versteht man, wie hoch die individuelle Belastung für einen Einzelnen ist, das Lernmaterial zu verstehen. Diese Belastung ist sehr wichtig und muss gefördert werden, da beispielsweise eine zu niedrige Lernbezogene-Belastung genau wie eine zu hohe Lernbezogene-Belastung nicht förderlich für den Lernprozess sein können.\n"
     ]
    }
   ],
   "source": [
    "# Run LSA\n",
    "lsa = LsiModel(corpus, num_topics = 300)\n",
    "\n",
    "# Build similarity matrix\n",
    "similarities = MatrixSimilarity(lsa[corpus], num_features=lsa.num_terms)\n",
    "\n",
    "# Get most similar text compared to origin wikipedia entry\n",
    "best_candidate = similarities[lsa[wikipedia_tuples]]\n",
    "\n",
    "# Show me the best candidate\n",
    "print(sorted(enumerate(best_candidate), key=lambda x: x[1], reverse=True))[0:20]\n",
    "\n",
    "print(documents[18])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
